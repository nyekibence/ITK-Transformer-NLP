{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ffec6e3552248afb859e0207a2d9d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f94268f7596c4276afa54d7eda7cb940",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_403d5de6f653408080cd5fba8441aa96",
              "IPY_MODEL_417721c721f147d58769855f6cd76560",
              "IPY_MODEL_ffae2bdc5c994a7489dea5ba0d5aa85a"
            ]
          }
        },
        "f94268f7596c4276afa54d7eda7cb940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "403d5de6f653408080cd5fba8441aa96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca9a1de93d594cbbbe964a8822add1b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c8b283499304b2f9fb875e6d5117af0"
          }
        },
        "417721c721f147d58769855f6cd76560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d66a97f2a2bd4e34b89abb7ec8fe5748",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_902564a271334438bb65258ff2c78285"
          }
        },
        "ffae2bdc5c994a7489dea5ba0d5aa85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c7bc754818448b7a3426ae18bfd8725",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00,  5.99ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_087acebd63b74aadbe1a0dc8fdc89186"
          }
        },
        "ca9a1de93d594cbbbe964a8822add1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c8b283499304b2f9fb875e6d5117af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d66a97f2a2bd4e34b89abb7ec8fe5748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "902564a271334438bb65258ff2c78285": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c7bc754818448b7a3426ae18bfd8725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "087acebd63b74aadbe1a0dc8fdc89186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <h1><center><b>Ismerkedés a Transformer-alapú modellekkel</b></h1></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UAvj2HTiyNtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mi a Transfromer architektúra és mire jó?**\n",
        "\n",
        "* Enkóder-dekóder architektúra\n",
        "* Alrétegei: Attention, előrecsatolt alréteg\n",
        "* Az alrétegek reziduális blokkok\n",
        "* Hatékony szekvenciafeldolgozásra tervezték konvolúciós és rekurrens rétegek nélkül"
      ],
      "metadata": {
        "id": "9PmyzfwT0Yx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1aysNBj04B4hvTw0ft8VZAAsmJB1poUK3\"/><p>Forrás: Viswani et al. (2017)</P></center>\n"
      ],
      "metadata": {
        "id": "Zfo-Y8W5x80u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Transformer-rétegek előtt: tokenbeágyazások\n",
        "\n",
        "Abszolút **pozíciós kódolás**:\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = sin \\left ( \\frac{pos}{10000^{\\frac{2i}{\\text{d_model}}}} \\right )\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = cos \\left ( \\frac{pos}{10000^{\\frac{2i}{\\text{d_model}}}} \\right )\n",
        "$$\n",
        "\n",
        "$pos$: pozíció, $i$: a beágyazás $i$-edik dimenziója, *d_model*: beágyazásméret\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1TLhPgksmaKDZLK-fBjRBSCAbCDJ_imAs\"/><p>Forrás: Kernes (2021)</p></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "* A maximálisan kódolható pozíciók számát előre rögzíteni kell\n",
        "* A tanítóanyagból hiányzó szekvenciahosszokat is jól modellezi\n",
        "* Relatív pozíciós kódolás: bármilyen nemnegatív (a maximális szekvenciahossznál nem nagyobb) $k$-ra $PE_{pos+k}$ felírható $PE_{pos}$ lineáris függvényeként.\n",
        "\n",
        "A pozíciók relatív reprezentációjára vonatkozó állítás azt jelenti, hogy bármely $\\omega_i$ súlyra és $k$-ra létezik olyan $M_{i,k}$ mátrix, amelyre\n",
        "\n",
        "$$\n",
        "M_{i,k} \\left (\n",
        "\\begin{matrix}\n",
        "sin(\\omega_ipos) \\\\\n",
        "cos(\\omega_ipos)\n",
        "\\end{matrix}\n",
        "\\right ) = \\left (\n",
        "\\begin{matrix}\n",
        "sin(\\omega_i(pos + k)) \\\\\n",
        "cos(\\omega_i(pos + k))\n",
        "\\end{matrix}\n",
        "\\right )\n",
        "$$\n",
        "\n",
        "$M_{i,k}$ ki is számítható:\n",
        "\n",
        "$$\n",
        "M_{i,k} = \\left (\n",
        "\\begin{matrix}\n",
        "cos(\\omega_ik)) && sin(\\omega_ik) \\\\\n",
        "-sin(\\omega_ik)) && cos(\\omega_ik)\n",
        "\\end{matrix}\n",
        "\\right )\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "Részletesebben:  Kazemnejad (2019)\n",
        "\n",
        "Implementáció: Kernes (2021)"
      ],
      "metadata": {
        "id": "XMl_PT-A7Mr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Az Attention mechanizmus\n",
        "\n",
        "**Bemenet**: az beágyazó réteg vagy az előző Transformer réteg kiemenete, egy `(batch_méret, szekvencia_hossz, beágyazás_méret)` alakú tenzor\n",
        "\n",
        "**Kiemenet**: a bemettével azonos alakú tenzor módosított beágyazásokkal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hae4BSPqTZm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U33dWCSTl7SC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672e3802-77d0-423b-b504-6a9e616cba5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50TokenizerFast,\n",
        "    pipeline,\n",
        "    set_seed\n",
        ")"
      ],
      "metadata": {
        "id": "HYmFgW0Rmtyn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bemenetet megszorozzuk három súlymátrixszal, ezek a _query_ ($W^Q$), _key_ ($W^K$) és a _value_ ($W^V$). A szorzatokat jelöljük rendre a következőképpen: $Q, K, V$\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1jKfz_TCnLvAwGXLC0t5MSKlHFPXSVQUK\"/><p>Forrás: Alammar (2018)</p></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "Legyen $d_k$ a K mátrix oszlopainak száma (beágyazásméret), $Z$ pedig at Attention kimenete. Ekkor az Attention kiszámításának képlete: \n",
        "\n",
        "$$\n",
        "Z = softmax \\left(\\frac{QK^T}{\\sqrt{d_K}} \\right)V\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=15CxXpzRRzs_wSNhaPt3Dhtn22W0CmEjE\"/><p>Forrás: Alammar (2018)</p></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "$Q, K$ és $V$ sorainak száma változó lehet.\n",
        "\n",
        "Az Attention tehát a **vektorreprezentációk skalárszorzatával** kísérli meg megragadni a tokenek közötti kapcsolatokat.\n",
        "\n",
        "Az osztás $\\sqrt{d_k}$-val akkor különösen fontos, amikor $d_k$ nagy: ekkor a ekkor a skalárszorzatok is nagyok lehetnek, a szoftmax gradiensei pedig kicsik.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G7jvC-6fZkmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a vector\n",
        "x = torch.tensor([5.67, 3.97, 7.79, 4.52], requires_grad=False)\n",
        "# softmax without dividing by `sqrt(d_k)`\n",
        "s1 = nn.functional.softmax(x, dim=-1)\n",
        "# softmax after dividing by `sqrt(d_k)`\n",
        "s2 = nn.functional.softmax(x / 2.0, dim=-1)\n",
        "\n",
        "print(s1, s2, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLLb5bq87iX2",
        "outputId": "edd40947-d620-43aa-a1dc-bc4c4e243fbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1017, 0.0186, 0.8475, 0.0322])\n",
            "tensor([0.2051, 0.0876, 0.5919, 0.1154])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A várt kimenetet ($Z$-t) a $V$-vel való szorzás után kapjuk meg. Az Attention bemenetét $X$-szel jelölve azt látjuk, hogy mind $X$, mind $Z$ sorai tokenreprezentációk. $Z$ reprezentációi azonban már **kontextuális információt** tartalmaznak. \n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1-o200diL3-DtVyt0rCzr8k7lRvinHxwA\"/><p>Forrás: Alammar (2018)</p></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "A **Multi-Head Attention** az Attention kiszámítását több, kisebb rejtett méretű Attention-fejben végzi. Ez lehetővé teszi, hogy a modell a különböző fejekben más-más jellemzőket tanuljon.\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1PlCXkTrhECnSmUxRHHA1Yx8AFySoFuaP\"/><p>Forrás: Vaswani et al. (2017)</p></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1V2FmIfzmeOuYEH4hLL0HDSb3FEzVWyTO\"/><p>Forrás: Alammar (2018)</p></center>"
      ],
      "metadata": {
        "id": "7CmL3Evlcnwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "  \"\"\"A dot product self-attention implementation\"\"\"\n",
        "\n",
        "  def __init__(self, d_x: int, d_k: int, d_v: int) -> None:\n",
        "    \"\"\"Layer initialization\n",
        "\n",
        "    Args:\n",
        "      d_x: Input embedding size\n",
        "      d_k: Hidden size of the query and key matrices\n",
        "      d_v: Hidden size of the value matrix\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Linear(d_x, d_k)\n",
        "    self.w_key = nn.Linear(d_x, d_k)\n",
        "    self.w_value = nn.Linear(d_x, d_v)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def _calculate_attention(\n",
        "      self,\n",
        "      q_tensor: torch.Tensor,\n",
        "      k_tensor: torch.Tensor, \n",
        "      v_tensor: torch.Tensor,\n",
        "      mask: torch.Tensor\n",
        "  )-> torch.Tensor:\n",
        "    \"\"\"Calculate Attenetion output if the Q, K, V tensors are given\n",
        "\n",
        "    Args:\n",
        "      q_tensor: The query tensor of shape `(batch_size, sequence_length,\n",
        "        key_hidden_size)`\n",
        "      k_tensor: The key tensor of shape `(batch_size, sequence_length,\n",
        "        key_hidden_size)`\n",
        "      v_tensor: The value tensor of shape `(batch_size, sequence_length,\n",
        "        value_hidden_size)`\n",
        "      mask: A binary tensor of shape `(batch_size, sequence_length)`, where\n",
        "        zeros indicate padding tokens\n",
        "    \n",
        "    Returns:\n",
        "      The attention output tensor of shape `(batch_size, sequence_length,\n",
        "        value_hidden_size)`\n",
        "    \"\"\"\n",
        "    if mask.dim() == 2:\n",
        "      mask = mask[:, None, :]\n",
        "      mask = (1.0 - mask) * -10000.0\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
        "    dots = torch.matmul(q_tensor, torch.transpose(k_tensor, -2, -1))\n",
        "    dots = dots + mask\n",
        "    attn_strengths = self.softmax(dots)\n",
        "    outputs = torch.matmul(attn_strengths, v_tensor)\n",
        "    return outputs\n",
        "\n",
        "  def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Forward pass implementation\n",
        "    \n",
        "    Args:\n",
        "      x: An input tensor of shape `(batch_size, sequence_length,\n",
        "        embedding_size)` or `(batch_size, sequence_length, num_attn_heads,\n",
        "        attn_head_size)`\n",
        "      mask: A binary tensor of shape `(batch_size, sequence_length)`.\n",
        "        Zeros indicate padding tokens\n",
        "      \n",
        "    Returns: \n",
        "      A tensor of shape `(batch_size, sequence_length,\n",
        "        value_hidden_size)`\n",
        "    \"\"\"\n",
        "    query = self.w_query(x)\n",
        "    key = self.w_key(x)\n",
        "    value = self.w_value(x)\n",
        "    return self._calculate_attention(query, key, value, mask)\n"
      ],
      "metadata": {
        "id": "KBOVNOklCKFW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHDotProductAttention(DotProductAttention):\n",
        "  \"\"\"Multi-head dot product attention implementation\"\"\"\n",
        "\n",
        "  def __init__(self, d_x: int, d_k: int, d_v: int, num_attn_heads: int) -> None:\n",
        "    \"\"\"Layer initialization\n",
        "\n",
        "    Args:\n",
        "      d_x: Input embedding size\n",
        "      d_k: Hidden size of the query and key matrices\n",
        "      d_v: Hidden size of the value matrix\n",
        "      num_attn_heads: The number of attention heads\n",
        "    \"\"\"\n",
        "    if d_k % num_attn_heads != 0 or d_v % num_attn_heads != 0:\n",
        "      raise ValueError(\"The hidden sizes `d_k` and `d_v` should be multiples \"\n",
        "                       \"of `num_attn_head`\")\n",
        "    super().__init__(d_x, d_k, d_v)\n",
        "    self._num_attn_heads = num_attn_heads\n",
        "    self._per_head_d_k = d_k // num_attn_heads\n",
        "    self._per_head_d_v = d_v // num_attn_heads\n",
        "  \n",
        "  def _calculate_attention(\n",
        "      self,\n",
        "      q_tensor: torch.Tensor,\n",
        "      k_tensor: torch.Tensor, \n",
        "      v_tensor: torch.Tensor,\n",
        "      mask: torch.Tensor\n",
        "  )-> torch.Tensor:\n",
        "    \"\"\"Calculate multi-head self-attention with the query, key, value\n",
        "    tensors given\n",
        "    \"\"\"\n",
        "    if mask.dim() == 2:\n",
        "      mask = mask[:, None, None, :]\n",
        "      mask = (1.0 - mask) * -10000.0\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid mask shape: {mask.shape}\")\n",
        "\n",
        "    batch_seq_heads = k_tensor.shape[:-1] + (self._num_attn_heads,)\n",
        "    q_tensor = torch.reshape(\n",
        "        q_tensor, batch_seq_heads + (self._per_head_d_k,)).transpose(1, 2)\n",
        "    k_tensor = torch.reshape(\n",
        "        k_tensor, batch_seq_heads + (self._per_head_d_k,)).transpose(1, 2)\n",
        "    v_tensor = torch.reshape(\n",
        "        v_tensor, batch_seq_heads + (self._per_head_d_v,)).transpose(1, 2)\n",
        "    \n",
        "    dots = torch.matmul(q_tensor, torch.transpose(k_tensor, -2, -1))\n",
        "    dots = dots + mask\n",
        "    attn_strengths = self.softmax(dots)\n",
        "    head_outputs = torch.matmul(attn_strengths, v_tensor).transpose(1, 2)\n",
        "    return torch.reshape(head_outputs, batch_seq_heads[:2] + (-1,))\n"
      ],
      "metadata": {
        "id": "E_NndNGzkZJl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 6, 8, dtype=torch.float32)\n",
        "print(f\"The original tensor:\\n{x}\", end=\"\\n\\n\")\n",
        "mask = torch.tensor([[1, 1, 1, 1, 0, 0], [1]*6], dtype=torch.float32)\n",
        "attn_model = MHDotProductAttention(d_x=8, d_k=8, d_v=8, num_attn_heads=2)\n",
        "print(f\"The modified tensor after Attention:\\n{attn_model(x, mask)}\")"
      ],
      "metadata": {
        "id": "pUnwwPZCOzsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53095ced-ced1-4dca-fbe9-5fbb67bec2a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original tensor:\n",
            "tensor([[[ 5.0283e-02, -2.8718e-01,  5.9291e-01, -2.1073e+00,  8.8631e-01,\n",
            "           2.5183e+00,  1.1344e+00,  3.2931e-01],\n",
            "         [-2.8812e-01,  5.9181e-01,  2.8666e+00,  1.0814e+00, -4.6995e-01,\n",
            "           1.6620e+00,  8.6864e-01, -1.0366e+00],\n",
            "         [ 2.8497e-03,  5.1901e-01, -9.9473e-02, -3.5825e-01, -5.2426e-01,\n",
            "          -8.6292e-03, -8.0892e-01,  1.2498e-01],\n",
            "         [ 7.6107e-01,  1.4409e-01, -5.9433e-01,  1.5065e-01,  3.2622e-02,\n",
            "          -9.8909e-01, -5.5912e-02, -4.9544e-01],\n",
            "         [-5.1751e-01, -1.3121e+00,  6.5042e-01,  5.0176e-01, -1.3434e+00,\n",
            "          -8.7413e-01,  1.2172e+00,  2.8059e-01],\n",
            "         [-1.1149e+00,  2.4540e-02, -8.8860e-01,  1.3537e+00,  9.7080e-02,\n",
            "          -5.7155e-01,  1.2084e+00,  1.8728e+00]],\n",
            "\n",
            "        [[ 4.4819e-01,  8.4787e-01,  5.8240e-01, -1.1143e+00, -1.0977e+00,\n",
            "          -1.5650e+00,  4.0007e-01, -2.0731e+00],\n",
            "         [ 1.1057e+00, -8.0421e-01,  2.3043e+00, -7.4986e-01, -5.1368e-01,\n",
            "           7.0227e-01, -1.3792e-01,  4.0781e-01],\n",
            "         [ 4.5020e-01, -1.4955e+00, -3.4068e-02,  7.9215e-01, -5.7746e-01,\n",
            "           6.6600e-01, -1.8212e+00, -3.0758e-01],\n",
            "         [-1.3041e-01,  4.6946e-01, -7.4329e-02,  1.3424e+00, -1.8969e+00,\n",
            "          -2.2903e-01,  1.3398e+00, -1.1958e-01],\n",
            "         [ 1.3005e+00,  4.1058e-01, -8.3306e-01,  2.6534e-02, -1.2633e+00,\n",
            "          -2.1626e-01, -4.1317e-02, -1.0840e+00],\n",
            "         [-2.1336e+00,  3.5801e-01, -8.6673e-01, -1.4333e+00,  6.7660e-01,\n",
            "          -4.1760e-01, -3.9891e-01,  8.5532e-01]]])\n",
            "\n",
            "The modified tensor after Attention:\n",
            "tensor([[[ 2.9854e-01, -1.6346e-01, -1.3750e-01, -4.2248e-01, -1.5385e-01,\n",
            "          -4.3953e-02,  1.7772e-01,  3.0174e-02],\n",
            "         [ 5.5786e-01, -3.7572e-01, -3.3722e-01, -5.0494e-01,  3.8322e-01,\n",
            "          -3.9513e-02,  3.6134e-01,  2.5808e-01],\n",
            "         [ 2.2908e-01, -1.2643e-01, -7.0010e-02, -4.0314e-01, -1.9582e-02,\n",
            "          -6.3104e-02,  3.4984e-01,  8.3560e-02],\n",
            "         [ 3.6421e-01, -2.6961e-01, -1.3611e-01, -4.5914e-01, -1.3745e-01,\n",
            "          -7.6345e-02,  3.7417e-01,  3.4246e-02],\n",
            "         [ 1.6370e-01, -1.0292e-01,  1.4184e-02, -3.9370e-01, -2.1476e-01,\n",
            "          -6.5087e-02,  3.2395e-01, -8.7423e-03],\n",
            "         [-6.5333e-03,  1.0595e-01,  8.4589e-02, -3.2333e-01, -1.8137e-01,\n",
            "          -6.9877e-02,  3.8593e-01, -1.1623e-04]],\n",
            "\n",
            "        [[-2.7050e-01,  4.9071e-01,  1.2237e-01, -3.0141e-01, -9.8339e-02,\n",
            "          -5.0568e-02, -8.5287e-02,  3.1780e-01],\n",
            "         [-2.3509e-01,  6.7378e-01, -2.5348e-01, -3.5105e-01, -1.6299e-01,\n",
            "           2.3445e-02, -6.6142e-02,  3.3867e-01],\n",
            "         [-2.1929e-01,  6.4166e-01, -3.3981e-01, -3.4590e-01, -2.2813e-01,\n",
            "          -8.4980e-02,  8.2432e-02,  2.5929e-01],\n",
            "         [-1.9794e-01,  3.1524e-01,  1.3397e-01, -3.5557e-01, -9.1965e-02,\n",
            "          -1.5349e-01, -7.3497e-02,  2.7268e-01],\n",
            "         [-2.1851e-01,  4.7289e-01, -4.1986e-02, -4.4818e-01, -2.2856e-01,\n",
            "          -1.0725e-01,  1.0991e-02,  2.8079e-01],\n",
            "         [ 5.3815e-03,  4.9775e-02,  5.5230e-01, -6.0941e-02, -4.8495e-02,\n",
            "          -8.3529e-02, -2.2333e-02,  2.4807e-01]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention súlyok kinyerése egy előtanított modellből**"
      ],
      "metadata": {
        "id": "OOsbtcUPjD1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_model = MBartForConditionalGeneration.from_pretrained(\n",
        "    \"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "for name, weight in seq2seq_model.named_parameters():\n",
        "  print(name)\n",
        "\n",
        "del seq2seq_model"
      ],
      "metadata": {
        "id": "7U6NsX6Ti7pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, _ in attn_model.named_parameters():\n",
        "  print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxoZKe6YkvX-",
        "outputId": "69a6cfcb-b7a8-4d60-d1b9-66e45756683d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_query.weight\n",
            "w_query.bias\n",
            "w_key.weight\n",
            "w_key.bias\n",
            "w_value.weight\n",
            "w_value.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Betanított modell használata gépi fordításra**\n",
        "\n",
        "Az alábbi példa a következőket fogja bemutatni:\n",
        "\n",
        "* A bemenet előkészítése gépi fordításhoz\n",
        "* Az mBART finomhangolt fordítómodell betöltése\n",
        "* A modell meghívása a fordítás generálásához\n",
        "\n",
        "Ehhez a Hugging Face `transformers` könyvtárát fogjuk használni."
      ],
      "metadata": {
        "id": "7mmC-q1ki2G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_sents() -> Dataset:\n",
        "  \"\"\"Create a dataset with the input sentences\"\"\"\n",
        "  sents = {\n",
        "      \"id\": list(range(5)),\n",
        "      \"sent\": [\n",
        "           \"Nearly ten years had passed since the Dursleys had woken up to \"\n",
        "             \"find their nephew on the front step, but Privet Drive had hardly \"\n",
        "             \"changed at all.\",\n",
        "           \"The sun rose on the same tidy front gardens and lit up the brass \"\n",
        "             \"number four on the Dursleys' front door; it crept into their \"\n",
        "             \"living room, which was almost exactly the same as it had been on \"\n",
        "             \"the night when Mr. Dursley had seen that fateful news report \"\n",
        "             \"about the owls.\",\n",
        "           \"Only the photographs on the mantelpiece really showed how much \"\n",
        "             \"time had passed.\",\n",
        "           \"Ten years ago, there had been lots of pictures of what looked like \"\n",
        "             \"a large pink beach ball wearing different-colored bonnets - but \"\n",
        "             \"Dudley Dursley was no longer a baby, and now the photographs \"\n",
        "             \"showed a large blond boy riding his first bicycle, on a carousel \"\n",
        "             \"at the fair, playing a computer game with his father, being \"\n",
        "             \"hugged and kissed by his mother.\",\n",
        "           \"The room held no sign at all that another boy lived in the house, \"\n",
        "             \"too.\"     \n",
        "      ]\n",
        "  }\n",
        "  # Text from Harry Potter and the Philosopher's Stone by J. K. Rowling\n",
        "  dataset = Dataset.from_dict(sents)\n",
        "  print(f\"An example from the dataset:\\n{dataset[0]['sent']}\")\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def tokenize_dataset(\n",
        "    sent_dataset: Dataset,\n",
        "    tokenizer: MBart50TokenizerFast,\n",
        "    batch_size: int\n",
        ") -> DataLoader:\n",
        "  \"\"\"Tokenize a dataset\n",
        "  \n",
        "  Args:\n",
        "    sent_dataset: A dataset that contains a field named `sent`\n",
        "    tokenizer: A pre-trained tokenizer model\n",
        "    batch_size: The batch size to use\n",
        "  \n",
        "  Returns:\n",
        "    The tokenized dataset as a PyTorch `DataLoader` \n",
        "  \"\"\"\n",
        "  sent_field = \"sent\"\n",
        "  old_fields = sent_dataset.features.keys()\n",
        "  if sent_field not in old_fields:\n",
        "    raise KeyError(f\"The dataset should contain a field named `{sent_field}`\")\n",
        "  if not isinstance(batch_size, int) or batch_size <= 0:\n",
        "    raise ValueError(\"`batch_size` should be a positive integer\")\n",
        "\n",
        "  def tokenize_example(example):\n",
        "     return tokenizer(example[sent_field], padding=True)\n",
        "  \n",
        "  sent_dataset = sent_dataset.map(\n",
        "      tokenize_example,\n",
        "      batched=True,\n",
        "      batch_size=batch_size,\n",
        "      remove_columns=list(old_fields)\n",
        "  )\n",
        "  sent_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "  dataloader = DataLoader(sent_dataset, batch_size=batch_size)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "def translate_sents(\n",
        "    model: MBartForConditionalGeneration,\n",
        "    tokenizer: MBart50TokenizerFast,\n",
        "    dataloader: DataLoader,\n",
        "    target_lang: str\n",
        ") -> List[str]:\n",
        "  \"\"\"Translate sentences\n",
        "  \n",
        "  Args:\n",
        "    model: The MT model\n",
        "    tokenizer: A pre-trained tokenizer model\n",
        "    dataloader: A PyTorch `DataLoader` with the fields\n",
        "      (`input_ids`, `attnetion_mask`)\n",
        "    target_lang: Target lanugage identifier string\n",
        "  \n",
        "  Returns:\n",
        "    A list of strings, the translations\n",
        "  \"\"\"\n",
        "  outputs = []\n",
        "  for input_dict in dataloader:\n",
        "    gen_tokens = model.generate(\n",
        "      **input_dict,\n",
        "      forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]\n",
        "    )\n",
        "    res = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
        "    outputs.append(res)\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "QVWHqWQ2kaqd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_translation(\n",
        "    dataset: Dataset,\n",
        "    model_name: str,\n",
        "    batch_size: int,\n",
        "    target_lang: str\n",
        ") -> List[List[str]]:\n",
        "  \"\"\"Full data flow: translate sentences\"\"\"\n",
        "  tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "  tokenizer.src_lang = \"en_XX\"\n",
        "  dataloader = tokenize_dataset(dataset, tokenizer, batch_size)\n",
        "\n",
        "  model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "  translations = translate_sents(model, tokenizer, dataloader, target_lang)\n",
        "  return translations"
      ],
      "metadata": {
        "id": "wZOHuS2soASh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_input_sents()\n",
        "res = do_translation(\n",
        "    dataset=dataset,\n",
        "    model_name=\"facebook/mbart-large-50-many-to-many-mmt\",\n",
        "    batch_size=2,\n",
        "    target_lang=\"de_DE\"\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "for src_sent, trg_sent in zip(\n",
        "    (example[\"sent\"] for example in dataset),\n",
        "    (decoded_sent for batch in res for decoded_sent in batch)):\n",
        "  print(f\"{src_sent}\\n{trg_sent}\", end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "Mvcfmg1p4Ohn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "6ffec6e3552248afb859e0207a2d9d40",
            "f94268f7596c4276afa54d7eda7cb940",
            "403d5de6f653408080cd5fba8441aa96",
            "417721c721f147d58769855f6cd76560",
            "ffae2bdc5c994a7489dea5ba0d5aa85a",
            "ca9a1de93d594cbbbe964a8822add1b3",
            "8c8b283499304b2f9fb875e6d5117af0",
            "d66a97f2a2bd4e34b89abb7ec8fe5748",
            "902564a271334438bb65258ff2c78285",
            "5c7bc754818448b7a3426ae18bfd8725",
            "087acebd63b74aadbe1a0dc8fdc89186"
          ]
        },
        "outputId": "8d888287-a65b-4160-d4ec-25582cfb2950"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example from the dataset:\n",
            "Nearly ten years had passed since the Dursleys had woken up to find their nephew on the front step, but Privet Drive had hardly changed at all.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ffec6e3552248afb859e0207a2d9d40",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearly ten years had passed since the Dursleys had woken up to find their nephew on the front step, but Privet Drive had hardly changed at all.\n",
            "Fast zehn Jahre waren vergangen, seit die Dursleys aufgewacht hatten, um ihren Neffen auf dem Vordersteg zu finden, aber Privet Drive hatte sich kaum verändert.\n",
            "\n",
            "The sun rose on the same tidy front gardens and lit up the brass number four on the Dursleys' front door; it crept into their living room, which was almost exactly the same as it had been on the night when Mr. Dursley had seen that fateful news report about the owls.\n",
            "Die Sonne stieg auf den gleichen ordentlichen Vorgarten und beleuchtete die Messing Nummer vier an der Dursleys Vordertür; sie kletterte in ihr Wohnzimmer, das war fast genau dasselbe, wie es war in der Nacht gewesen, als Herr Dursley den Schicksalsbericht über die Eulen gesehen hatte.\n",
            "\n",
            "Only the photographs on the mantelpiece really showed how much time had passed.\n",
            "Nur die Fotos auf demmantelstück zeigten wirklich, wie viel Zeit vergangen war.\n",
            "\n",
            "Ten years ago, there had been lots of pictures of what looked like a large pink beach ball wearing different-colored bonnets - but Dudley Dursley was no longer a baby, and now the photographs showed a large blond boy riding his first bicycle, on a carousel at the fair, playing a computer game with his father, being hugged and kissed by his mother.\n",
            "Vor zehn Jahren gab es viele Bilder von einem großen rosa Strandball, der verschiedene Farben trug - aber Dudley Dursley war nicht mehr ein Baby, und jetzt zeigten die Fotos einen großen blonden Jungen, der auf seinem ersten Fahrrad, auf einer Fassade auf der Messe fuhr, mit seinem Vater ein Computerspiel spielte, umarmt und von seiner Mutter ge küssen wurde.\n",
            "\n",
            "The room held no sign at all that another boy lived in the house, too.\n",
            "Der Raum hielt überhaupt kein Zeichen dafür, dass auch ein anderer Junge im Haus wohnte.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset\n",
        "del res"
      ],
      "metadata": {
        "id": "NNz_5YQ6CT0z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A Hugging Face** `transformers` **könyvtára**\n",
        "\n",
        "Vessünk egy pillantást a `PyTorch`-ban implementált `GPT-2` [dokumentációjára](https://huggingface.co/docs/transformers/model_doc/gpt2)"
      ],
      "metadata": {
        "id": "OHOs2T4tBvq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A `pipeline example`\n",
        "# See also the original example and tutorial:\n",
        "# https://huggingface.co/tasks/text-generation\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Studying language models is great, because\",\n",
        "          max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "Av_5VrerxAkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8fd766-6ff1-4b73-d663-1bd2334dbfc8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Studying language models is great, because you can actually understand it. In fact, this is one of the reasons I use it. All these different'},\n",
              " {'generated_text': \"Studying language models is great, because there's a lot of flexibility and you can take it from there. I have read books on language, but\"},\n",
              " {'generated_text': 'Studying language models is great, because it has to be a bit of extra work.\\n\\nIn a way, I prefer to work on a'},\n",
              " {'generated_text': \"Studying language models is great, because you will see these approaches come in handy. It's also where you can make a conscious effort to learn how\"},\n",
              " {'generated_text': \"Studying language models is great, because it helps you find the right language! Even if you're not fluent in either one, you should keep that\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cikkek, olvasnivalók\n",
        "\n",
        "Transformer:\n",
        "* Vaswani et al. (2017): _Attention Is All You Need_ [Link](https://arxiv.org/abs/1706.03762)\n",
        "* Jay Alammar (2018): _The Illustrated Transformer_ (blog) [Link](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "Transformers könyvtár:\n",
        "* Wolf et al. (2020): _Transformers: State-of-the-Art Natural Language Processing_ [Link](https://aclanthology.org/2020.emnlp-demos.6/)\n",
        "\n",
        "Az Attention kezdetei:\n",
        "* Bahdanau et al. (2014): _Neural Machine Translation by Jointly Learning to Align and Translate_ [Link](https://arxiv.org/abs/1508.04025)\n",
        "* Sutskever et al. (2014): _Sequence to Sequence Learning with Neural Networks_ [Link](https://arxiv.org/abs/1409.3215)\n",
        "* Loung et al. (2015): _Effective Approaches to Attention-based Neural Machine Translation_ [Link](https://arxiv.org/abs/1508.04025)\n",
        "\n",
        "Pozicókódolás:\n",
        "* Shaw et al. (2018): _Self-Attention with Relative Position Representations_ [Link](https://arxiv.org/abs/1803.02155)\n",
        "* Huang et al. (2018): _Music Transformer: Generating Music with Long-Term Structure_ [Link](https://arxiv.org/abs/1809.04281)\n",
        "* Huang et al. (2020): _Improve Transformer Models with Better Relative Position Embeddings_ [Link](https://arxiv.org/abs/2009.13658)\n",
        "* Wang et al. (2020): _What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding_ [Link](https://arxiv.org/abs/2010.04903)\n",
        "* Amirhossein Kazemnejad (2019): _Transformer Architecture: The Positional Encoding_ (blog) [Link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
        "* Jonathan Kernes (2021): _Master Positional Encoding: Part I_ (Towards Data Science) [Link](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)\n",
        "\n",
        "A skalárszorzat-hasonlóság szemléltetése:\n",
        "* Tivadar Danka (2021): _How the Dot Product Measures Similarity_ (Towards Data Science) [Link](https://towardsdatascience.com/how-the-dot-product-measures-similarity-b3e16e22beda)"
      ],
      "metadata": {
        "id": "P749BqX6_uUB"
      }
    }
  ]
}