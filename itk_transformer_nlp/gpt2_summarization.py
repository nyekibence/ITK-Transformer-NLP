#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Fine-tune GPT2 on text summarization
using the `Webis-TLDR-17 Corpus Dataset`
"""

from argparse import ArgumentParser, Namespace
from typing import Dict, Any, Iterable, List
from functools import partial

import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from datasets import Dataset

from itk_transformer_nlp.transformer_qa import load_jsonl_dataset
from itk_transformer_nlp.encoder_cola import (
    get_general_training_args,
    fine_tune_transformer,
    check_positive_int
)


def get_summarization_args() -> Namespace:
    """Get command line arguments for text summarization"""
    parser = ArgumentParser(description="Command line arguments for fine-tuning GPT2 "
                                        "on text summarization.")
    parser.add_argument("train_dataset", type=load_jsonl_dataset,
                        help="Path to the `jsonlines` training dataset.")
    parser.add_argument("val_dataset", type=load_jsonl_dataset,
                        help="Path to the `jsonlines` validation dataset.")
    get_general_training_args(parser)
    parser.add_argument("--body-text-col", dest="body_text_col", default="normalizedBody",
                        help="The column in the dataset that contains the text which should be "
                             "summarized. Defaults to `'normalizedBody'`.")
    parser.add_argument("--summary-col", dest="summary_col", default="summary",
                        help="The column in the dataset that contains the summaries. "
                             "Defaults to `'summary'`.")
    parser.add_argument("--body-max-length", dest="body_max_length",
                        type=check_positive_int, default=512,
                        help="Maximal sequence length of the summaries in terms of tokens. "
                             "Summaries longer than this limit will be truncated. "
                             "Defaults to `512`.")
    parser.add_argument("--summary-max-length", dest="summary_max_length", default=128,
                        help="Maximal sequence length of the summaries in terms of tokens. "
                             "Summaries longer than this limit will be truncated. "
                             "Defaults to `128`.")
    return parser.parse_args()


def tokenize_summarization_dataset(
        dataset: Dataset,
        tokenizer: GPT2Tokenizer,
        body_text_col: str,
        summary_col: str,
        body_max_length: int,
        summary_max_length: int,
        batch_size: int,
        sep_string: str = "TLDR",
        input_ids_col: str = "input_ids",
        labels_col: str = "labels",
        padding_token_id: int = 0,
        ignore_label: int = -100
) -> DataLoader:
    """Tokenize a summarization dataset.

    Args:
        dataset: The dataset that is to be tokenized.
        tokenizer: A pre-trained tokenizer.
        body_text_col: The dataset column that contains the text bodies, i.e. the texts
            which should summarized.
        summary_col: The dataset column that contains the summaries.
        body_max_length: Maximal sequence length of the text bodies in terms of tokens.
            Texts longer than this limit will be truncated.
        summary_max_length: Maximal sequence length of the summaries in terms of tokens.
            Summaries longer than this limit will be truncated.
        batch_size: Batch size parameter for the `DataLoader`.
        sep_string: A string appended to the text bodies. This string can be used as a
            separator when the text bodies and the summaries are concatenated.
            Defaults to `'TLDR'`.
        input_ids_col: Input token ID key in the dictionaries generated by the output
            `DataLoader`. Defaults to `'input_ids'`.
        labels_col: Label key in the dictionaries generated by the output
            `DataLoader`. Defaults to `'labels'`.
        padding_token_id: Padding token ID to pad the input token ID sequences. It can be
            the ID of any token from the tokenizer vocabulary. Defaults to `0`.
        ignore_label: Padding token to pad the label sequences. `GPT2LMHeadModel` ignores the
            label `-100`, so set it to this value if you are planning to use `GPT2LMHeadModel`.
            Defaults to `-100`.

    Returns:
        A data loader that generates padded data batches as dictionaries with the keys
            `input_ids_col` and `labels_col`.
    """
    sep_string_tokens = tokenizer.encode(sep_string)
    body_max_length = body_max_length - len(sep_string)
    assert body_max_length > 0

    def tok_func(example: Dict[str, Any]) -> Dict[str, List[int]]:
        source_text_tokens = tokenizer.encode(
            example[body_text_col])[:body_max_length]
        target_text_tokens = tokenizer.encode(
            example[summary_col])[:summary_max_length]
        source_text_tokens.extend(sep_string_tokens)
        source_text_labels = [ignore_label] * (len(source_text_tokens) - 1)
        target_text_labels = target_text_tokens + [ignore_label]
        return {
            input_ids_col: source_text_tokens + target_text_tokens,
            labels_col: source_text_labels + target_text_labels
        }

    def collate(
            batch: Iterable[Dict[str, torch.Tensor]],
            padding_values: Dict[str, int]
    ) -> Dict[str, torch.Tensor]:
        batch_dict = {}
        for key, padding_value in padding_values.items():
            batch_tensor = pad_sequence(
                [seq[key] for seq in batch],
                batch_first=True,
                padding_value=padding_value
            )
            batch_dict[key] = batch_tensor
        return batch_dict

    padding_values_dict = {
        input_ids_col: padding_token_id,
        labels_col: ignore_label
    }
    partial_collate = partial(collate, padding_values=padding_values_dict)
    dataset = dataset.map(tok_func)
    dataset.set_format("torch", columns=[input_ids_col, labels_col])
    data_loader = DataLoader(dataset, batch_size=batch_size,
                             collate_fn=partial_collate)
    return data_loader


def main() -> None:
    """Main function"""
    model_name = "gpt2"
    args = get_summarization_args()
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenization_kwargs = {
        "dataset": args.train_dataset,
        "tokenizer": tokenizer,
        "body_text_col": args.body_text_col,
        "summary_col": args.summary_col,
        "body_max_length": args.body_max_length,
        "summary_max_length": args.summary_max_length,
        "batch_size": args.batch_size
    }
    train_data_loader = tokenize_summarization_dataset(
        dataset=args.train_dataset, **tokenization_kwargs)
    val_data_loader = tokenize_summarization_dataset(
        dataset=args.val_dataset, **tokenization_kwargs)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model = fine_tune_transformer(
        model=model,
        train_data_loader=train_data_loader,
        val_data_loader=val_data_loader,
        num_epochs=args.num_epochs,
        learning_rate=args.lr,
        weight_decay=args.weight_decay
    )
    if args.model_save_path is not None:
        model.save_pretrained(args.model_save_path)


if __name__ == "__main__":
    main()
