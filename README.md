# ITK-Transformer-NLP
Bevezetés az NLP-be Transformer-alapú modellekkel

## Tematika

Jelölések:

* (E) elméleti áttekintés
* (K) kód, implementáció bemutatása
* (E+K) elmélet és kód, elvi megoldások és implementáció
* (P) kidolgozott példa, a munkafolyamat bemutatása a bemenet előkészítésétől a kimenet értelmezéséig

A gyakorlatok a kidolgozott példákhoz hasonlóak, kiegészítendő kódot tartalmaznak, az önálló gyakorlást segítik.

### Ismerkedés a Transformer-alapú modellekkel
2022. március 7.
• Mi a Transformer architektúra és mire jó? (E)
• Az Attention mechanizmus (E+K)
• Attention súlyok kinyerése egy előtanított modellből (K)
• Betanított modell használata gépi fordításra (P)
• A Hugging Face transformers könyvtára (K)
• Gyakorlat: Question answering enkóder-dekóder modellel
 
### Enkóderek
2022. március 21.
• Transfer learning az NLP-ben: előnyök és hátrányok (E)
• A BERT modellek: az előtanítás (E+K)
• Finomhangolás szentimentanalízisre (P)
• Sentence-BERT tanítása NLI dataseten (E+K)
• Gyakorlat: Enkódermodell finomhangolása CoLA dataseten

### Dekóderek
2022. március 28.
• „Mi a következő token?”: A nyelvmodellezés elvei (E)
• Dekódolás Top-k random mintavételezéssel (E+K)
• Szöveggenerálás GPT-2 modellel (P)
• Az időre és memóriára optimalizált Sparse Transformer Attention mechanizmusa (E+K)
• Gyakorlat: Összefoglalógenerálás dekódermodellel
 
### Haladó témák
2022. április 4.
• Hiperparaméter-hangolás (E+K)
• Knowledge distillation (E+K)
• Cross-lingual few-shot és zero-shot (E)
• Cross-lingual few-shot osztályozás (P)
• Gyakorlat: Multilingual knowledge distillation
